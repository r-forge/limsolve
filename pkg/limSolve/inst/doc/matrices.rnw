\documentclass[article,nojss]{jss}
\DeclareGraphicsExtensions{.pdf,.eps}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Add-on packages and fonts
%%\usepackage{mathptmx}
%%\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{amssymb}
%%\usepackage{array} % tabel commands
%%\usepackage{xspace}
%%\usepackage{verbatim}
%%\usepackage[english]{babel}
%\usepackage{mathptmx}
%\usepackage{helvet}
%%\usepackage[T1]{fontenc}
%%\usepackage[latin1]{inputenc}
%%\renewcommand{\ttdefault}{lmtt}
%%\renewcommand{\familydefault}{\rmdefault}
%%\usepackage[T1]{fontenc}
%%\usepackage[latin1]{inputenc}
%%\usepackage{geometry}


\newcommand{\noun}[1]{\textsc{#1}}
%% Bold symbol macro for standard LaTeX users
\providecommand{\boldsymbol}[1]{\mbox{\boldmath $#1$}}

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\newcommand{\ls}{\textbf{\textsf{limSolve }}}
\newcommand{\li}{\textbf{\textsf{LIM }}}
\newcommand{\R}{\proglang{R }}
\title{Matrix calculations in \proglang{R}}
\Plaintitle{Matrix calculations in R}

\Keywords{linear equations, matrices, quadratic programming, linear programming, \proglang{R}}

\Plainkeywords{linear equations, matrices, quadratic programming, linear programming, R}


\author{Karline Soetaert\\
Centre for Estuarine and Marine Ecology\\
Netherlands Institute of Ecology\\
The Netherlands}

\Plainauthor{Karline Soetaert}

\Abstract{This document gives an overview of \R functions that deal with matrices and sets of
linear equations. It includes linear and quadratic programming.
}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Karline Soetaert\\
  Centre for Estuarine and Marine Ecology (CEME)\\
  Netherlands Institute of Ecology (NIOO)\\
  4401 NT Yerseke, Netherlands
  E-mail: \email{k.soetaert@nioo.knaw.nl}\\
  URL: \url{http://www.nioo.knaw.nl/ppages/ksoetaert}\\
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% R/Sweave specific LaTeX commands.
%% need no \usepackage{Sweave}
%\VignetteIndexEntry{Matrices}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Begin of the document
\begin{document}
\SweaveOpts{engine=R,eps=FALSE}
\SweaveOpts{keep.source=TRUE}

<<preliminaries,echo=FALSE,results=hide>>=
library("Matrix")
library("SparseM")
library("limSolve")
options(prompt = "> ")

@

\maketitle

\section{Introduction}
Many of the linear algebra functions are part of the R core package \citep{R2008}. However, some
additional packages are recommended:
\begin{itemize}
\item \pkg{Matrix}   \citep{Matrix}. Matrix calculations for full and sparse matrices.
\item \pkg{SparseM}  \citep{SparseM}. Matrix calculations for arbitrary sparse matrices.
\item \pkg{MASS}     \citep{MASS}. Generalised inverse, null spaces of matrices
\item \pkg{limSolve} \citep{Soetaert08b}. Least squares, quadratic and linear programming
\item \pkg{quadprog} \citep{quadprog}. Quadratic programming
\item \pkg{lpSolve}  \citep{lpSolve}. Linear and integer programming
\end{itemize}

\section{Matrix algebra - basics}
A \textbf{matrix} is a rectangular array of numbers.
 A matrix with m rows and n columns is said to be of order m x n;
 (m,n) is also referred to as the dimension of the matrix.

 In expanded form, a matrix is written with coefficients aij for each entry;
 the first subscript refers to the rows, the second to the columns.
 A matrix is often symbolized with an uppercase, bold-faced letter.

\[ \textbf{A}_{2x2}  =
\left[ {\begin{array}{*{20}c}
   {a11} & {a12}  \\
   {a21} & {a22}  \\
\end{array}} \right]
\]

A matrix with column order one is called a \textbf{column vector}.
A \textbf{row vector} is a matrix with one row and several columns.

A \textbf{diagonal matrix} is one for which the values off diagonal are 0.

The \textbf{identity matrix I} is a special form of a diagonal matrix where the diagonal elements are 1.

A matrix is called \textbf{triangular} if all elements on one side of the diagonal are 0.
An upper triangular matrix has non-zero elements only above the diagonal;
a lower triangular matrix has non-zero elements below the diagonal:
\[
\left[ {\begin{array}{*{20}c}
   2 & 0 & 0 & 0  \\
   3 & 4 & 0 & 0  \\
   5 & 6 & 7 & 0  \\
   8 & 9 & {10} & {11}  \\
\end{array}} \right]
\]

In R, matrices can be created in several ways:
\begin{itemize}
\item	By means of R-function \code{matrix}
\item	By means of R-function \code{diag} which constructs a diagonal matrix
\item	The functions \code{cbind} and \code{rbind} add columns and rows to an existing matrix, or to another vector
\end{itemize}

The following statement creates a matrix A, with two rows, and, as there are four elements, two columns.

<<>>=
A <-matrix(nrow=2,data=c(1,2,3,4))
A
@
Note that the data are inputted as a vector (using the c() function).

By default, R fills a matrix column-wise (see the example above). However, this can easily be overruled, using parameter \code{byrow}:
<<>>=
(M <-matrix(nrow=4, ncol=3, byrow=TRUE, data=1:12))
@
Matrices can also be created by combining (binding) vectors, rowwise (\code{rbind}) or columnwise (\code{cbind}):
<<>>=
V <- 0.5:5.5
rbind(V,sqrt(V))
@
A diagonal matrix is created using R-function \code{diag}:
<<>>=
diag(nrow=2)
diag(x= 1:2,nrow =2, ncol=3)
@
Upper and lower triangular matrices can be created with \code{upper.tri} and \code{lower.tri}

<<>>=
m2 <- matrix(nr=4,data=1:20)
m2[lower.tri(m2)]<-0
m2
@
\section{Matrix operations}
The \textbf{product} between matrices A and B, written as AB, or $A \cdot B$ is only defined if the
number of columns of A equals the number of rows of B.
The product of a matrix with order m x n with a matrix with order n x p gives a new matrix with order m x p and
whose ij-th element equals: $\sum\limits_{k = 1}^n {a_{ik} } b_{kj}$.

In R, matrix multiplication is done using \code{\%*\%}.
<<>>=
(A <- matrix(nrow=2,ncol=3,data=6:1))
B <- matrix(nrow=3,ncol=4,data=1:12)
A%*%B
@

The identity matrix plays the same role as the number 1 in scalar notation, this is:
$AI = A = IA$
<<>>=
A%*%diag(3)
@

Multiplication of a matrix \textbf{A} with a \textbf{scalar}, c, results in a new matrix with the same dimension as
\textbf{A} and whose ij-th element equals $c \cdot a_{ij}$
<<>>=
2*A
@

\textbf{Addition and subtraction} can only be performed between matrices of the same order.
It is done element by element: the resulting matrix has the same order and its ij-th element equals $a_{ij}+b_{ij}$.
<<>>=
A+A
@

The sum of the elements that fall on the diagonal, $\sum\limits_{i = 1}^n {a_{ii} }$ is called the \textbf{trace} of the matrix.
<<>>=
sum(diag(A))
@

The \textbf{transpose} of an mxn matrix A, is a nxm matrix denoted by $A^T$ or $A'$ and having rows
identical to the columns of A and vice-versa:
\[A^T=
\left[ {\begin{array}{*{20}c}
   {a11} & {a21}  \\
   {a12} & {a22}  \\
\end{array}} \right]
\]

Note: $(A^T)^T=A$; $(A+B)^T=A^T +B^T$; $(A \cdot B)^T=B^T \cdot A^T$

\R-function \code{t()} takes the matrix transpose.
<<>>=
(A <- matrix(nrow=2,data=1:4))
t(A)
@

The \textbf{inverse} of a matrix A is the unique matrix $A^{-1}$ which, when multiplied with A produces the identity matrix I:
$A \cdot A^{-1} = I$.
The inverse can only be taken from square matrices; not all square matrices have an inverse.
Note that $(cA)^{-1}=c^{-1} \cdot A^{-1}$; $(A \cdot B)^{-1}=B^{-1} \cdot A^{-1}$

\R-function \code{solve()} takes the matrix inverse.
<<>>=
solve(A)
solve(A)%*%A
@

An \textbf{orthogonal matrix} T is a matrix whose transpose equals its inverse:
 \[T^T = T^{-1}\]
Thus: $T^T \cdot T = I$, the identity matrix.
Orthogonal matrices can be created by the QR transformation (see below).
<<>>=
(A  <- matrix(nrow=2,ncol=2,data=1))
(AO <- qr.Q(qr(A), complete = TRUE))
 AO%*%t(AO)
@
\section{Eigenvalues and eigenvectors, determinants}
Given a square (m x m) real matrix A, an \textbf{eigenvalue} $\lambda$ is a scalar for which:
$A \cdot x = \lambda x$  for $x \neq 0$:

\[
\begin{array} {*{20}l}
a_{11}\cdot x_1+a_{12}\cdot x_2  + \ldots a_{1n}\cdot x_n=\lambda \cdot x_1\\
a_{21}\cdot x_1+a_{22}\cdot x_2  + \ldots a_{2n}\cdot x_n=\lambda \cdot x_2\\
\ldots \\
a_{n1}\cdot x_1+a_{n2}\cdot x_2  + \ldots a_{nn}\cdot x_n= \lambda \cdot x_n
\end{array}
\]

x is called the \textbf{(right) eigenvector} of A for the eigenvalue $\lambda$;
the eigenvector is defined only up to a scaling factor.

Note: the eigenvalues of $A^{-1}$ are the reciprocals of the eigenvalues of A.

For symmetric A the eigenvalues are real (otherwise they might be complex numbers).
A matrix A is said to be \textbf{nonsingular} if it has only nonzero eigenvalues.
A matrix is \textbf{positive definite} if all of its eigenvalues are positive.

The \textbf{determinant} of a matrix is the product of its eigenvalues.

In R the eigenvalue and (right) eigenvectors of a matrix are calculated by funcion \code{eigen}.
It returns a list that contains both the eigenvalues (\$values) and the eigenvectors (\$vectors).
The latter are represented by the columns of the matrix.
<<>>=
(er <- eigen(A))
A%*%er$vectors[,1]-er$values[1]*er$vectors[,1]
A%*%er$vectors[,2]-er$values[2]*er$vectors[,2]
@
The \textbf{left eigen vectors} of A equal the right eigenvectors of its transpose.
<<>>=
(el <- eigen(t(A)))
el$vectors[,1]%*%A-el$values[1]*el$vectors[,1]
el$vectors[,2]%*%A-el$values[2]*el$vectors[,2]
@
The determinant of A is calculated with \R-function \code{det}.
<<>>=
det(A)
@

\section{linear equations}
The n linear equations in the m unknowns x
\[
\begin{array} {*{20}l}
a_{11} \cdot x_1+a_{12} \cdot x_2  + \ldots a_{1n} \cdot x_n=b_1 \\
\ldots                                                           \\
a_{m1} \cdot x_1+a_{m2} \cdot x_2  + \ldots a_{mn} \cdot x_n=b_m
\end{array}
\]

can be written in matrix form:
\[\mathbf{A}\cdot \mathbf{x}=\mathbf{b}\]

Where \textbf{A(mxn)} is called the coefficient matrix, vector \textbf{x} contains the unknowns and
\textbf{b} is called the right hand side.

Vectors \textbf{$x_1$}, ... \textbf{$x_N$} are called \textbf{linearly dependent} if there exist numbers
$l_1$,... $l_N$, all non-zero, such that:
\[l_1 \cdot \mathbf{x_1}+  ... + l_N \cdot \mathbf{x_N} = \mathbf{0}\]

The vectors are \textbf{linearly independent} if no such numbers can be found.

The \textbf{rank} of a matrix is the minimum of (the maximum number of linearly independent rows or maximum number of linearly independent columns).
It is denoted by r(A) or rank(A). Note that for a matrix A(m x n): $0 \leq r(A) \leq \min(m,n)$


A \textbf{basis of the null-space} of a matrix, M, is a matrix N such that:
\[ N^t \cdot M = 0\]
and where N has the maximum number of linearly independent columns.


If A is square and positive definite, the solution of the linear system $Ax = b$ is done using R's function \code{solve}:
<<>>=
A <- matrix(nrow=2,data=1:4)
B <- c(5,6)
X <- solve(A,B)

A%*%X-B
@
To estimate the rank of matrix, it is easiest to use the QR decomposition.
Alternatively, the singular value decomposition can be used (see below).
<<>>=
A <- matrix(nrow=4,ncol=4,data=c(1:8,6,8,10,12,1:4))
A    # col3=col1+col2; col4=col1
qr(A)$rank
@

\R-pakage \pkg{MASS} provides a function to estimate a basis for the null space of a matrix:
<<>>=
Null(A)
t(Null(A))%*%A
@

\section {Kronecker product}
The kronecker product of two matrices ($\otimes$) is an operation on two matrices of arbitrary size resulting in a block matrix.

Example:

\[
\left[{\begin{array} {*{50}l}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
a_{31} & a_{32}
\end{array}}\right]
\otimes
\left[{\begin{array}{*{50}l}
b_{11} & b_{12} & b_{13} \\
b_{21} & b_{22} & b_{23}
\end{array} }\right]
=
\left[{\begin{array} {*{50}l}
a_{11} b_{11} & a_{11} b_{12} & a_{11} b_{13} & a_{12} b_{11} & a_{12} b_{12} & a_{12} b_{13} \\
a_{11} b_{21} & a_{11} b_{22} & a_{11} b_{23} & a_{12} b_{21} & a_{12} b_{22} & a_{12} b_{23} \\
a_{21} b_{11} & a_{21} b_{12} & a_{21} b_{13} & a_{22} b_{11} & a_{22} b_{12} & a_{22} b_{13} \\
a_{21} b_{21} & a_{21} b_{22} & a_{21} b_{23} & a_{22} b_{21} & a_{22} b_{22} & a_{22} b_{23} \\
a_{31} b_{11} & a_{31} b_{12} & a_{31} b_{13} & a_{32} b_{11} & a_{32} b_{12} & a_{32} b_{13} \\
a_{31} b_{21} & a_{31} b_{22} & a_{31} b_{23} & a_{32} b_{21} & a_{32} b_{22} & a_{32} b_{23}
\end{array}}\right]
 \]
In \R, this operation can be performed using \code{kronecker}.

Kronecker products can be used to estimate the coefficients of a linear equation matrix.
Consider the problem where the linear equality equation $Ax = b$, has to be solved for the unknown coefficients of the matrix A, i.e. x is known, A is unknown.

The set of equations with the coefficients of A as the unknowns is given by:
\[ (\mathbf{x}^T \otimes \mathbf{I}) \cdot \mathbf{A} = \mathbf{b}\]

where \textbf{I} is the identity matrix of appropriate dimension, and vec(\textbf{A}) is a vector containing the coefficients of A, columnwise.

<<>>=
x <- c(1,2)
kronecker(t(x),diag(2))
@

\section{Singular value decomposition}

The \textbf{singular value decomposition} (SVD), sometimes called the spectral decomposition,
 is a very important decomposition.

It forms the basis of the generalized inverse of a matrix (see below).

An m x n matrix A can be decomposed as:
\[A = U \cdot D\cdot V^T\]
where

$U_{(m x p)}$ and $V_{(n x p)}$ are column-orthonormal matrices,
i.e. they satisfy :

$U \cdot U^T=I$ and $V \cdot V^T=I$,

I is the identity matrix of appropriate dimension.

U contains the left singular vectors, V the right singular vectors and

D(p x p)  is a diagonal matrix containing the 'singular values $\sigma_i$, which are positive values,
generally arranged in decreasing order, i.e. $D = diag(\sigma_1,\sigma_2,\ldots \sigma_p)$ with $\sigma_1\geq \sigma_2 \geq , \ldots \geq \sigma_p$.
Here p is the minimum of (m,n), i.e. the initial 'guess' of the rank of matrix A.

The rank r can be estimated by counting the number of non-zero singular values in D.

In order to count a singular value as zero, we have to decide on the tolerance to use, generally this is taken as a very small number
(e.g. the root of machine precision in the order of 1e-8).

The approximation \[A \simeq U_{(m x r)} D_{(n x r)}^{-1} V_{(n x r)}^T\] is called the \emph{low-rank or truncated singular value decomposition} of A.
If singular values are truncated at sufficiently low values, the approximation will be near exact.

\section{The generalized inverse}
The  \textbf{generalized inverse} of a matrix A, denoted by $A^-$ is one for which \[A \cdot A^- \cdot A=A\]

In contrast to the inverse $A^{-1}$, which only exists for certain square matrices,
the generalized inverse always exists although it need not be unique.
If the inverse of A exists, the generalized inverse is equal to the inverse.

It is constructed by singular value decomposition as follows:

If $A = U\cdot D \cdot V^T$

Then $A^- =V_{(n x r)} \cdot D_{(n x r)}^{-1} \cdot U_{(m x r)}^T$

And where $D_{(r x r)}^{-1}= diag(1/\sigma_1,1/\sigma_2,\ldots 1/\sigma_r)$.

Here the matrices $U_{(m x r)}$, $D_{(r x r)}$ and $V_{(n x r)}$ use the actual (or better estimated)
rank r of the matrix A, which can be smaller than p.

The generalized inverse forms the basis of solving any set of linear equations.
A particular solution of the equation $A \cdot x = b$, where A is an m x n matrix, using the generalized inverse is written as:
$x = A^- \cdot b$

We consider 3 cases.
\begin{itemize}
\item
 $m=n=rank(A)$, there are as many equations as unknowns.
In this case, $A^- = A^{-1}$ and the solution is unique and given by:
$x = A^{-1}\cdot b$

\item $m>n\geq rank(A)$, there are more equations than unknowns.
The equations are said to be \emph{overdetermined}.
The generalized inverse solution is the one that minimizes the sum of squared residuals,
i.e. $min (\|A x - b\|^2$).

\item $m<n\leq rank(A)$, there are more unknowns than equations.
The system is said to be \emph{underdetermined}, and there are an infinite number of solutions.
The particular generalized inverse solution is the one that minimizes the sum of squared x's,
i.e.$\min (\| x \|^2)$. This solution is often called the 'parsimonious' solution.

A more general set of solutions is given by:
$X = A^- \cdot B + (I- A^- \cdot A) \cdot z$
Where z is a vector of length n, with arbitrary numbers
\end{itemize}

The \emph{'data' resolution matrix} $A \cdot A^-$ describes how well the equations match the right hand side.
The elements on the diagonal denote the importance of the equations in determining the values of the unknowns.
Values less than 1 indicate that the equations are not fully independent.

The \emph{'model' resolution matrix} $A^-\cdot A$ characterizes whether the unknowns can be uniquely determined.
A value on the diagonal less than 1 indicates that the unknowns are not fully constrained.


In the \R-script below, a rectangular matrix is created (1st line),
the singular value decomposition taken and shown (2nd line).
The machine precision (3rd line) is then used to select the singular values
that are sufficiently large (i.e. larger than the precision, 4th line).
The rank of the matrix is then the number of sufficiently large singular values (5th line).
(but it is simpler to use the QR decomposition for rank calculation).


<<>>=
A <- matrix(nrow=4,ncol=3,data=c(1:8,6,8,10,12))
(s      <- svd(A))
tol     <- sqrt(.Machine$double.eps)
nonzero <- which(s$d > tol * s$d[1])
(rank   <- length(nonzero))
qr(A)$rank   # this is easier...
@
The generalized inverse is taken, and right- and left multiplied with A,
which returns the original matrix.
<<>>=
gA  <- ginv(A)

A %*% gA %*% A
@

After creating a vector B, which forms the right hand side of the linear set
of equations $A\cdot x=B$, the parsimonious solution is calculated
The third line checks if the residuals are indeed 0.
Finally, the resolutions of equations and variables are estimated.
<<>>=
B <- 0:3
(X <- gA %*% B)
A %*% X - B                           # should be zero

res.eq   <- diag(A %*%gA)
res.var  <- diag(gA%*% A)
@
The previous calculations can also be done using functions from package \ls.
<<>>=
Solve(A,B)
resolution(A)
@

\section{Other matrix decompositions}

\subsection{QR}
One important decomposition is the QR decomposition, which decomposes a
rectangular matrix A into an upper triangular matrix R and an orthogonal matrix Q:
\[A = Q \cdot R\]
As Q is orthogonal, it follows that $Q^T \cdot Q=I$.
The QR decomposition was used above to estimate the rank of a matrix.
<<>>=
A <- matrix(nrow=4,ncol=3,data=c(1:8,6,8,10,12))
qrA<-qr(A)
qr.Q(qrA)               # Q
qr.R(qrA)               # R
qr.Q(qrA) %*% qr.R(qrA) #Q R = A
@

\subsection{Cholesky decomposition}
The cholesky decomposition takes the 'square root' of a square matrix, i.e.
it transforms a matrix in the product of a matrix with its transpose:
\[A = R^T \cdot R\]
Similarly as a real root of a negative number does not exist, matrix A
has to obey certain characteristics in order for its square root to exist:
this is A has to be positive definite.

Note that sometimes, the cholesky decomposition may fail when it should not.
Adding a tiny value on the diagonal helps to remedy this:
<<>>=
(A  <- matrix(nrow=2,ncol=2,data=1))
#chol(A)                                    # fails
diag(A) <- diag(A) +  .Machine$double.eps
(sqA    <- chol(A))                         # works
t(sqA)%*%sqA
@

\subsection{lu (lower-upper) decomposition}
The LU decomposition writes a matrix as the product of a permutation of a lower
triangular matrix and of an upper triangular matrix.

In \R, LU decomposition is available by function \code{lu} from package \pkg{Matrix}.

Function \code{lu} creates the lu factorization in the form:
\[A = P \cdot L \cdot U \]

where P is a permutation matrix, L is lower triangular with unit diagonal elements (lower trapezoidal if m > n),
and U is upper triangular (upper trapezoidal if m < n).

This package makes use of the S4 class system, and does not produce output as a list.
Rather, the requested information can be retrieved by calling the appropriate "methods".
In addition, it does not take as input an ordinary "matrix", but a class of type "Matrix".

Here is a function based on \code{lu}, but using the S3 class system \footnote{which, in this case, I find easier to work with}:
<<>>=
LU <- function(A)
{
ll<-expand(lu(Matrix(A)))
ll$L <- as.matrix(ll$L)
ll$U <- as.matrix(ll$U)
ll$P <- as.matrix(ll$P)
ll
}

(A <- matrix(nr=3,data=runif(9)))
(lud <-LU(A))
lud$P%*%lud$L%*%lud$U   #equal to A
@
(Note: this only works for square matrices A)


\section{linear equality and inequality equations}
There are many problems where linear equations are supplemented with linear inequality constraints that have to be met.
Formally, the problem can be specified as:
\[      \min(f(\mathbf{x})) \qquad or \qquad \max(f(\mathbf{x}))\]
subject to:
\begin{align*}
      \mathbf{E}\cdot \mathbf{x}=\mathbf{f}      \\
      \mathbf{G}\cdot \mathbf{x}\geq\mathbf{h}
\end{align*}

where \textbf{x} is the vector with the unknowns and either \textbf{E} and/or \textbf{G} may be empty.
The function $f(\mathbf{x})$ which has to be minimized or maximized need not be linear.
\subsection{Quadratic programming}
In case the cost function is quadratic, the problem is called a least squares problem with equality and inequality conditions (LSEI):

\begin{align*}
      \min(\| \mathbf{A} \cdot \mathbf{x} - \mathbf{b} \|^2) \qquad (1)\\
      \mathbf{E}\cdot \mathbf{x}=\mathbf{f}      \qquad  (2)\\
      \mathbf{G}\cdot \mathbf{x}\geq\mathbf{h}   \qquad  (3)
\end{align*}

Some of the equations must be exactly satisfied ($\mathbf{Ex=f}$), whilst others do not ($\mathbf{Ax=b}$),
and where in addition certain inequality constraints need to be satisfied.


These problems are called "least squares" problems with equality and inequality conditions. They are solved by quadratic programming techniques.

The formalism $\| \|^2$, is also called the L2 norm. It can be rewritten as:
\[
\begin{array}{c}
 \left\| {Ax - b} \right\|^2  = \left( {Ax - b} \right)^T \left( {Ax - b} \right) \\
  = b^T b + x^T A^T Ax - 2b^T Ax \\
 \end{array}
\]

As the first term does not contain the unknown \textbf{x}, it can be ignored such that
\[
\begin{array}{l}
 \min \left\| {Ax - b} \right\|^2  = \min (\frac{{x^T Dx}}{2} - p^T x) \\
 D = A^T A \\
 p^T  = b^T A \\
 \end{array}
\]

Thus, LSEI problems can be inputted either using matrix \textbf{A} and vector \textbf{b} or matrix \textbf{D} and vector \textbf{p}.

Functions \code{lsei} from package \pkg{limSolve} uses the first formalism, while
function \code{solve.QP} uses the second formalism.

The following problem
\begin{center}
\[
\begin{array} {*{50}l}
{3 \cdot x_1} & {+ 2 \cdot x_2} &{+ x_3}         &{+ 4 \cdot x_4} &=& 2 \\
{x_1}         & {+ x_2 }        &{+ x_3}         &{+ x_4}         &=& 2 \\
\\
{2 \cdot x_1} & {+ x_2}         &{+ x_3}         &{+ x_4} &\geq& -1 \\
{-1 \cdot x_1}& {+ 3 \cdot x_2} &{+ 2 \cdot x_3} &{+ x_4} &\geq& 2 \\
{-1 \cdot x_1} & {} &{+ x_3}         &{} &\geq& 1 \\
\\
{2 \cdot x_1} & {+2 \cdot x_2}         &{+ x_3}         &{+6 \cdot x_4} &\simeq&1 \\
{x_1} & {- x_2}         &{+ x_3}         &{- x_4} &\simeq&2
\end{array}
\]
\end{center}
is implemented and solved in R as:
<<>>=
E <- matrix(ncol=4, byrow=TRUE,
            data=c(3,2,1,4,1,1,1,1))
F <- c(2,2)
A <- matrix(ncol=4, byrow=TRUE,
            data=c(2,2,1,6,1,-1,1,-1))
B <- c(1,2)
G <-matrix(ncol=4,byrow=TRUE,
           data=c(2,1,1,1,-1,3,2,1,-1,0,1,0))
H <- c(-1, 2, 1)

lsei(E=E,F=F,A=A,B=B,G=G,H=H)$X
@
\subsection{Linear programming}
In case the function to be minimised or maximise is linear, the problem can be
 solved with linear programming algorithms. They also assume that all x>0.

 \[      \min(\sum\limits_i {a_i x_i }) \qquad  or \qquad \max(\sum\limits_i {a_i x_i }) \]
subject to:

 \begin{align*}
      \mathbf{E}\cdot \mathbf{x}=\mathbf{f}      \qquad  \\
      \mathbf{G}\cdot \mathbf{x}\geq\mathbf{h}   \qquad  \\
      \mathbf{x_i} > 0
\end{align*}

A robust linear programming function, \code{lp} can be found in package \pkg{lpSolve}.

Package \pkg{limSolve} provides a wrapper around \code{lp} such that the input
is compatable with the input of function \code{lsei}.
Example:

\begin{center}
\[ \min(x_1 + 2 \cdot x_2 - 1 \cdot x_3 + 4 \cdot x_4) \]
\end{center}
subject to :
\begin{center}
\[  \begin{array} {*{50}l}
{3 \cdot x_1} & {+ 2 \cdot x_2} &{+ x_3}         &{+ 4 \cdot x_4} &=& 2 \\
{x_1}         & {+ x_2 }          &{+ x_3}         &{+ x_4}         &=& 2\\
\\
{2 \cdot x_1} & {+ x_2}         &{+ x_3}         &{+ x_4} &\geq& -1 \\
{-1 \cdot x_1}& {+ 3 \cdot x_2} &{+ 2 \cdot x_3} &{+ x_4} &\geq& 2 \\
{-1 \cdot x_1} & {} &{+ x_3}         &{} &\geq& 1 \\
\\
x_i  &\geq& 0 \qquad \forall i \\
\end{array} \]
\end{center}
is implemented in R as:
<<linp>>=
E <- matrix(ncol=4, byrow=TRUE,
            data=c(3,2,1,4,1,1,1,1))
F <- c(2,2)

G <-matrix(ncol=4,byrow=TRUE,
           data=c(2,1,1,1,-1,3,2,1,-1,0,1,0))
H <- c(-1, 2, 1)
Cost <- c(1,2,-1,4)
linp(E,F,G,H,Cost)
@


\clearpage

\begin{table*}[t]
\caption{Summary of matrix functions in \R}\label{tb:summ}
\centering
\begin{tabular}{l l l l}\\
{}                 & \R-Function     & Package  &Description\\
\hline
$A^T$              & t(A)            & base     &\\
$A^T \cdot B$      & crossprod(A,B)  & base     &\\
$A^T \cdot A$      & crossprod(A,A)  & base     &\\
$A \cdot B^T$      & tcrossprod(A,B) & base     &\\
$A \cdot A^T$      & tcrossprod(A,A) & base     &\\
$A \otimes B$      & kronecker(A,B)  & base     & kronecker product\\
solve $Ax=b$ for x & solve(A,B)      & base     & A=square, positive definite \\
solve $Ax=b$ for x & Solve(A,B)      & limSolve & any A\\
$A^{-1}$           & solve(A)        & base     & inverse, A=square, positive definite \\
$A^{-}$            & ginv(A)         & MASS     & Moore-Penrose generalised inverse \\
$A^{-}$            & Solve(A)        & limSolve & Moore-Penrose generalised inverse \\
{}                 & eigen(A)        & base     & eigen values and eigen vectors\\
{}                 & svd(A)          & base     & singular value decomposition of A\\
{}                 & det(A)          & base     & determinant\\
{}                 & chol(A)         & base     & Choleski factorization of A\\
{}                 & lu(A)           & Matrix   & LU decomposition of A\\
{}                 & qr(A)\$qr       & base     & QR decomposition of A\\
{}                 & qr(A)\$rank     & base     & rank of A \\

\hline
\end{tabular}
\end{table*}

\bibliography{vignettes}

\end{document}
